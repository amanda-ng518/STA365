{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf265e7",
   "metadata": {},
   "source": [
    "# Wai Yu Amanda, Ng\n",
    "## Student number: 1008217350\n",
    "## UTORid: ngwai11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11affb16",
   "metadata": {},
   "source": [
    "# 1. Describe how the posterior predictive distribution is created for mixture models \n",
    "\n",
    "According to the codes above, we\n",
    "\n",
    "1. Pick a posterior Markov chain and a draw from the chain\n",
    "2. Extract the posterior vector samples of mu, sigma and weight from the posterior sample drawn shown step 1\n",
    "3. Use the samples from step 2 to define a mixture model (Normal distributions with location mu_i (an entry from the mu vector) and standard deviation sigma_i (an entry from the sigma vector), each Normal component is multiplied to the w_i (an entry from the weight vector) by elementwise multiplication).\n",
    "4. By plotting the mixture model (mixed Normal model) out, we obstain the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7184abc",
   "metadata": {},
   "source": [
    "# 2. Describe how the posterior predictive distribution is created in general\n",
    "\n",
    "\n",
    "The posterior predictive distribution is the distribution of possible unobserved values conditional on the observed values.\n",
    "\n",
    "The posterior predictive distribution is created by marginalizing (i.e. integrating over $\\theta$) distribution of $\\tilde y$ given $\\theta$ times the posterior distribution of $\\theta$. That is\n",
    "\n",
    "$$p(\\tilde y| x) = \\int p(\\tilde y| \\theta) p(\\theta|x)d\\theta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848b87d",
   "metadata": {},
   "source": [
    "# 3. Have glance through [this](https://www.pymc.io/projects/examples/en/latest/case_studies/Missing_Data_Imputation.html) and then describe how, if you were doing a regression of $y$ on $X$ but $X$ had some missing values, you could perform a Bayesian analysis without throwing away the rows with missing values in $X$\n",
    "\n",
    "- **Hint: latent variables $v$ indicating the subpopulation are competely missing values that we simply treat as paramters to be inferred though posterior analysis... the same sort of thing can be done with missing values in data that need to be imputed... we should just be careful about the MCAR assumption...**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfcc8e3",
   "metadata": {},
   "source": [
    "**1. Full Information Maximum Likelihood**\n",
    "\n",
    "It uses maximum likelihood estimation to estimate the parameters of the multivariate normal regression model that could be best said to generate our observed data. It respects the fact that we have missing data in our original data set. The procedure works by partitioning the data into their patterns of “missing-ness” and treating each partition as contributing to the ultimate log-likelihood term that we want to maximise. We combine their contributions to estimate a fit for the multivariate normal distribution. We can then sample from the implied distribution to estimate other features of interest and test against the observed data.\n",
    "\n",
    "**2. Direct imputation of the missing values using the posterior predictive distribution.**\n",
    "\n",
    "It imputes the missing values for specific missing entries through the process of MCMC sampling. This is done by first imposing priors on the parameters of the regression model. Then, a vector of flat variables for the unobserved components (i.e. missing data) of the MvNormal is created. Next, a symbolic value (like a latent variables $v$) is created to combine observed data and missing variables. Lastly, a potential with the logp of the variable conditioned on the symbolic value defined above is added. Lastly, we conduct MCMC sampling from this model to impute the missing values.\n",
    "\n",
    "**3. Bayesian Imputation by Chained Equations**\n",
    "\n",
    "The previous imputation method treat each of the variables in our dataset as a collection drawn from the same distribution, this method serves as a more flexible method when there is a particular focal relationship that we’re interested in analysing. When there is variable that does not contain missing value, we have a joint distribution that can be decomposed and  can be split out into individual regression equations or more generally component models for each required conditional model. We can impute each of these equations in turn saving the imputed data set and feeding it forward into the next modelling exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90917b32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
